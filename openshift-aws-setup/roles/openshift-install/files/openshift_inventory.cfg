# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
nodes
{% if install_logging or install_metrics %}
nfs
{% endif %}
{% if install_gluster %}
glusterfs
{% endif %}

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
openshift_clusterid={{ namespace }}

# SSH user, this user should allow ssh based auth without requiring a password
ansible_ssh_user={{ remote_user_name }}
ansible_become=true

# If ansible_ssh_user is not root, ansible_sudo must be set to true
#ansible_sudo=true

deployment_type={{ platform }}
openshift_release=v{{ version }}
openshift_install_examples=true

# Set cloud provider
openshift_cloudprovider_kind=aws
openshift_cloudprovider_aws_access_key={{ lookup('env','AWS_ACCESS_KEY_ID') }}
openshift_cloudprovider_aws_secret_key={{ lookup('env','AWS_SECRET_ACCESS_KEY') }}
openshift_storageclass_parameters={'type': 'gp2', 'encrypted': 'false', 'zone': '"{{ availability_zone }}"'}

# We need a wildcard DNS setup for our public access to services
openshift_master_default_subdomain={{ wildcard }}

# Comment the following to enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '{{ htpasswd_path }}'}]

# Apply updated node defaults
#openshift_node_kubelet_args={'pods-per-core': ['10'], 'max-pods': ['250'], 'image-gc-high-threshold': ['90'], 'image-gc-low-threshold': ['80']}

{% if install_metrics %}
# See: https://docs.openshift.com/enterprise/latest/install_config/cluster_metrics.html
openshift_hosted_metrics_deploy=true
openshift_hosted_metrics_storage_kind=nfs
openshift_hosted_metrics_storage_access_modes=['ReadWriteOnce']
openshift_hosted_metrics_storage_nfs_directory=/srv
openshift_hosted_metrics_storage_nfs_options='*(rw,root_squash)'
openshift_hosted_metrics_storage_volume_name=metrics
openshift_hosted_metrics_storage_volume_size=2Gi
{% endif %}

{% if install_logging %}
# See: https://docs.openshift.com/enterprise/latest/install_config/aggregate_logging.html
openshift_hosted_logging_deploy=true
openshift_hosted_logging_storage_kind=nfs
openshift_hosted_logging_storage_access_modes=['ReadWriteOnce']
openshift_hosted_logging_storage_nfs_directory=/srv
openshift_hosted_logging_storage_nfs_options='*(rw,root_squash)'
openshift_hosted_logging_storage_volume_name=logging
openshift_hosted_logging_storage_volume_size=2Gi
{% endif %}

{% if install_prometheus %}
# See: https://docs.openshift.com/container-platform/3.7/install_config/cluster_metrics.html#openshift-prometheus
# openshift_hosted_prometheus_deploy=true
# openshift_prometheus_storage_type=pvc
# openshift_prometheus_alertmanager_storage_type=pvc
# openshift_prometheus_alertbuffer_storage_type=pvc
# Bug: https://bugzilla.redhat.com/show_bug.cgi?id=1522977
# openshift_prometheus_node_selector={"region":"infra"}
{% endif %}

# Comment this to prevent app nodes from being scheduled on master nodes
# [NOTE] Commenting this will cause app nodes to be scheduled on Gluster nodes
#osm_default_node_selector="region=primary"

# Disable checks
openshift_disable_check=docker_storage,docker_storage_driver,memory_availability

# Enable ntp on masters to ensure proper failover
openshift_clock_enabled=true

[masters]
{{ master_private_dns_name }} openshift_public_hostname={{ domain }}

[etcd]
{{ master_private_dns_name }}

{% if install_logging or install_metrics %}
[nfs]
{{ master_private_dns_name }} openshift_node_labels="{'region': 'infra', 'zone': 'default'}" openshift_schedulable=true
{% endif %}

# Host group for nodes, includes region info
[nodes]
{{master_private_dns_name}} openshift_node_labels="{'region': 'infra', 'zone': 'default'}" openshift_schedulable=true
{% for node in nodes_ip %}
{% if node.gluster %}
{{node.private_dns_name}}
{% else %}
{{node.private_dns_name}}
#{{node.private_dns_name}} openshift_node_labels="{'region': 'primary', 'zone': '{{ 'west' if loop.index is divisibleby 2 else 'east'}}'}"
{% endif %}
{% endfor %}

{% if install_gluster %}
[glusterfs]
{% for node in nodes_ip %}
{% if node.gluster %}
{{node.private_dns_name}} glusterfs_devices="[ '/dev/xvdc' ]"
{% endif %}
{% endfor %}
{% endif %}

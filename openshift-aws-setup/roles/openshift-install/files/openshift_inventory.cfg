# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
nodes
{% if install_nfs %}
nfs
{% endif %}
{% if install_gluster %}
glusterfs
{% endif %}

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
# SSH user, this user should allow ssh based auth without requiring a password
ansible_ssh_user={{ remote_user_name }}

# If ansible_ssh_user is not root, ansible_become must be set to true
ansible_become=true

openshift_clusterid={{ namespace }}
deployment_type={{ platform }}
openshift_release=v{{ version }}
openshift_install_examples=true

# Set cloud provider
openshift_cloudprovider_kind=aws
openshift_cloudprovider_aws_access_key={{ lookup('env','AWS_ACCESS_KEY_ID') }}
openshift_cloudprovider_aws_secret_key={{ lookup('env','AWS_SECRET_ACCESS_KEY') }}
openshift_storageclass_parameters={'type': 'gp2', 'encrypted': 'false', 'zone': '{{ availability_zone }}'}

# We need a wildcard DNS setup for our public access to services
openshift_master_default_subdomain={{ wildcard }}

# Enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '{{ htpasswd_path }}'}]

# Apply updated node defaults
#openshift_node_kubelet_args={'pods-per-core': ['10'], 'max-pods': ['250'], 'image-gc-high-threshold': ['90'], 'image-gc-low-threshold': ['80']}

# See: https://docs.openshift.com/enterprise/latest/install_config/cluster_metrics.html
{% if install_nfs and install_metrics %}
openshift_hosted_metrics_deploy=true
openshift_hosted_metrics_storage_kind=nfs
openshift_hosted_metrics_storage_access_modes=['ReadWriteOnce']
openshift_hosted_metrics_storage_nfs_directory=/exports
openshift_hosted_metrics_storage_nfs_options='*(rw,root_squash)'
openshift_hosted_metrics_storage_volume_name=metrics
openshift_hosted_metrics_storage_volume_size=10Gi
{% else %}
openshift_metrics_install_metrics=true
openshift_metrics_storage_kind=dynamic
{% endif %}

# See: https://docs.openshift.com/enterprise/latest/install_config/aggregate_logging.html
{% if install_nfs and install_logging %}
openshift_hosted_logging_deploy=true
openshift_hosted_logging_storage_kind=nfs
openshift_hosted_logging_storage_access_modes=['ReadWriteOnce']
openshift_hosted_logging_storage_nfs_directory=/exports
openshift_hosted_logging_storage_nfs_options='*(rw,root_squash)'
openshift_hosted_logging_storage_volume_name=logging
openshift_hosted_logging_storage_volume_size=10Gi
{% else %}
openshift_logging_install_logging=true
openshift_logging_storage_kind=dynamic
{% endif %}

{% if install_prometheus %}
# See: https://docs.openshift.com/container-platform/3.7/install_config/cluster_metrics.html#openshift-prometheus
# openshift_hosted_prometheus_deploy=true
# openshift_prometheus_storage_type=pvc
# openshift_prometheus_alertmanager_storage_type=pvc
# openshift_prometheus_alertbuffer_storage_type=pvc

# Bug in 3.7 https://bugzilla.redhat.com/show_bug.cgi?id=1522977 if you enabled the below
# openshift_prometheus_node_selector={"region":"infra"}
{% endif %}

{% if install_gluster }
# Prevent applications from being scheduled on gluster nodes
osm_default_node_selector="region=primary"
{% endif %}

# Enable ntp on masters to ensure proper failover
openshift_clock_enabled=true

# Disable checks
openshift_disable_check=docker_storage,docker_storage_driver,memory_availability

[masters]
{{ master_private_dns_name }} openshift_public_hostname={{ domain }}

[etcd]
{{ master_private_dns_name }}

{% if install_nfs %}
[nfs]
{{ master_private_dns_name }}
{% endif %}

# Host group for nodes, includes region info
[nodes]
{{master_private_dns_name}} openshift_node_labels="{'region': 'infra', 'zone': 'default'}" openshift_schedulable=true
{% for node in nodes_ip %}
{% if node.gluster %}
{{node.private_dns_name}}
{% else %}
{{node.private_dns_name}} openshift_node_labels="{'region': 'primary', 'zone': '{{ 'west' if loop.index is divisibleby 2 else 'east'}}'}"
{% endif %}
{% endfor %}

{% if install_gluster %}
[glusterfs]
{% for node in nodes_ip %}
{% if node.gluster %}
{{node.private_dns_name}} glusterfs_devices="[ '/dev/xvdc' ]"
{% endif %}
{% endfor %}
{% endif %}
